\documentclass[a4paper]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}

\title{Path Planning for Multi-Agent Teams using Rapidly-exploring Random Trees}

\author{Yash Bagla}


\begin{document}
\maketitle



\section{Introduction}
\label{sec:introduction}
Collision avoidance is central to many robotics applications, such as multi-agent coordination [1], autonomous navigation through human crowds [2], pedestrian motion prediction, and computer crowd simulation. Yet, finding collision-free, time efficient paths around other agents remains challenging, because it may require predicting other agents’ motion and anticipating interaction patterns, through a process that needs to be computationally tractable for real-time implementation.

This project presents a novel approach to address the challenge of planning paths for multi-agent systems operating in complex environments. The algorithm developed, Decentralized Multi-Agent Rapidly-exploring Random Tree (DMA-RRT), is an extension of the Closed-loop RRT (CLRRT) algorithm to the multi-agent case, retaining its ability to plan quickly even with complex constraints. Moreover, a merit-based token passing coordination strategy is developed to dynamically update the
planning order based on a measure of each agent’s incentive to replan, derived from the CLRRT. Agents with a greater incentive plan sooner, yielding a greater reduction of the global cost and greater improvement in the team’s overall performance[1].

The most straightforward approach to decentralized, multi-agent path planning is to allow all agents to continuously plan their own paths subject to constraints imposed by the other agents’ paths. While this has the benefit of being fully asynchronous, it may very quickly lead to conflicting plans if one agent modifies the constraints while another is planning.


\section{Problem Statement}
\label{sec:Problem Statement}
This project examines the problem of planning dynamically feasible paths through complex environments for a team of autonomous agents attempting to reach specified goal locations. As seen in any practical system, the agents are assumed to have constrained, nonlinear dynamics. Therefore, the agents may be incapable of following paths generated by simple path planners, such as piecewise-linear paths, making dynamic feasibility a crucial requirement. Hence, static or linear paths cannot be used - requires paths to be dynamically updated. Agents must also be able to navigate safely in cluttered or otherwise complex real-world environments. Furthermore, these paths must satisfy all constraints placed on the interactions between multiple agents, the most common example being avoiding the collision between agents, teams are assumed to consist of many agents that may be operating over a large area, precluding the use of centralized planners to satisfy these requirements. Finally, paths must be selected to minimize a given cost metric, such as fuel consumption or travel time, across all agents in the team.




\section{Previous Work}
\subsection{Path Planning}
Navigation has been the focus of various diverse scientific communities, ranging from computer vision and robotics to sociology and cognitive science, aiming at understanding and simulating human navigation but also at reproducing robotic navigation. There have been numerous path planning algorithms developed for autonomous systems, largely due to the fundamental role of path planning in any form of mobile robotics. These approaches are usually categorized as either discrete or continuous path planning algorithms, and methods from both categories have been used with considerable success in different applications.



\subsection{Path Planning using Constraint Solvers}
Discrete search methods [12] have been used extensively for robot path planning, but typically do not scale well to large problems with complex agent dynamics and environments. Mixed Integer Linear Programming (MILP) [3] and Model Predictive Control (MPC) [4] retain the benefits of optimization and can account for agent dynamics more easily. However, these approaches also scale poorly with the number of constraints imposed by the complex dynamics and environment. Potential field methods have also enjoyed considerable success in simple scenarios, but have difficulty in complex environments [5].

\subsection{Sampling Based Planners}
In recent years, Sampling-based motion planners, such as Probabilistic Roadmap (PRM) methods [6] and the Rapidly-exploring Random Tree (RRT) algorithm [7], have gained popularity. While the original RRT algorithm is a fast way to plan paths in complex, high-dimensional spaces, the real-time RRT algorithm proposed by Frazzoli [8] and extended by Kuwata et al. as Closed-loop RRT (CL-RRT) [9] is much more useful as an online path planner. As such, the work presented here employs the CL-RRT algorithm as the underlying path planner. For details on the CL-RRT algorithm, please refer to references [9] and [10].


\section{Decentralized Multi-Agent RRT}
\label{sec:Decentralized Multi-Agent RRT}
The Closed-loop RRT algorithm is very effective at planning paths quickly for a single agent subject to complex constraints. This section extends this planning capability to a team of cooperative agents by embedding the CL-RRT in a framework that manages interactions between multiple agents. As a result, for the multi-agent case only minimal changes to the CL-RRT algorithm are required. Furthermore, a merit-based token passing strategy is presented to improve system performance. This coordination strategy takes advantage of the CL-RRT mechanics and intelligently modifies the order of replanning an agents' plan. The resulting Decentralized Multi-Agent RRT (DMA-RRT) algorithm preserves the benefits of planning using CL-RRT while also improving collective performance and guaranteeing that the constraints imposed between agents are satisfied at all times.

\subsection{Model}
The environment that the team of agents operates in is assumed to be known and only contain static obstacles. Since the behavior of the agents can be predicted accurately given a model of dynamics and appropriate communication between agents, this assumption enables each agent to anticipate the state of the world for some time into the future and plan accordingly.

Each agent is assumed to have a model of its own dynamics to use as in the CL-RRT algorithm. Furthermore, each agent’s model is assumed to be known to all other agents, allowing for heterogeneous teams. While accurate models will improve performance, the robustness to modeling error [11] provided by the closed-loop nature of the CLRRT transfers to the DMA-RRT algorithm, potentially allowing even relatively simple models to work well.


\subsection{Merit-Based Token Passing}
In previous works the agents to cycle through a fixed planning order, regardless of whether an agent will see any substantial benefit from replanning.The alternative approach presented here relies on a measure of Potential Path Improvement (PPI) that reflects an agent’s incentive to replan. It represents the improvement in path cost an agent expects to see if allowed to update its plan next. The single subsystem update strategy is augmented with this PPI information to form a merit-based token passing strategy. Rather than iterating through a list of agents, a token is used to identify which agent is allowed to update its plan at each planning iteration. Agents without the token compute their PPI and broadcast these values as bids to be the next token holder. When the current token holder is finished replanning, it passes the token to the agent with the best bid, i.e. the greatest Potential Path Improvement. In the case of a tie, one of the agents in the tie is selected at random to be the next token holder. This effectively produces a dynamic planning order where agents that may benefit the most from replanning are able to do so sooner, without having to cycle through the list and wait for other agents that may have very little incentive to replan. As a result, agents that quickly find paths to reach their goals will typically generate higher bids more often, allowing them to act on these plans sooner and reach more of their goals.


\subsection{PPI from RRT}
Potential Path Improvement requires the agent to compare costs between the plan it is currently executing and the new plan it generates and want to switch to. It represents the difference in path cost between the agent’s current path and the best path in the tree is its PPI. For example, if the best path in the tree has a much lower cost than the path the agent is currently taking, the PPI would be large and it would be beneficial to replan soon to select the better path. In each planning iteration, the agent that is allowed to replan simply continues to run the CL-RRT algorithm and updates its plan accordingly. The other agents continue executing their previously selected plans, but also continue to grow their own tree using CL-RRT. As a result, it is easy for agents to identify new, lower-cost paths in the tree. In short, agent that benefits most from re-planning is given next chance to replan.


\section{Algorithm}
The DMA-RRT algorithm consists of two components that are run in parallel on each agent. The individual component handles the path planning of a particular agent, while the interaction
component handles all information received from other agents.

\subsection{Individual Component}
The agent is initialized with some dynamically feasible path, $p_{0}$, that satisfies all
constraints imposed by the environment and other agents for all time. One agent is initialized as the token holder, with all others initialized to $Have-Token \longleftarrow false$. Each agent grows the CL-RRT and identifies the best path, $p^{*}_{k}$, in the tree according to its cost function. The merit-based token passing strategy then determines if the agent will update its plan to $p^{*}_{k}$ and pass the token to winner (the agent with the best bid), or if it will bid to be the next token holder.

If the agent updates its plan, the constraints imposed on the other agents must also be updated accordingly. Due to the closed-loop nature of the RRT, any plan can be characterized by a sparse set of waypoints and closed-loop dynamics. Thus, it is sufficient for the agent to broadcast its new waypoints to allow the others to update constraints. This update occurs in the interaction component.

\begin{figure}
\centering
  \includegraphics[height=6.5cm]{Algo1}
  \caption{Algorithm 1 DMA-RRT, Individual component}
  \label{fgr:example}
\end{figure}
\begin{figure}
\centering
  \includegraphics[height=5cm]{algo2}
  \caption{Algorithm 2 DMA-RRT, Interaction component}
  \label{fgr:example}
\end{figure}
\subsection{Interaction Component}
The second component of the DMA-RRT algorithm, described in Algorithm 2, manages interaction between agents. Agents communicate via two messages, the first identifies the token winner and has a list of waypoints defining an updated plan, while the second is the bid (PPI value) for the token.

When the waypoints and winner message is received, the agent must update the constraints on its plans. Rather than receiving full trajectory information, the agent can simulate the other agent's trajectory along the received waypoints using a model of its dynamics. The CL-RRT can then treat this as a time-parameterized obstacle to ensure the path satisfies the inter-agent constraints. When the token winner receives this message, it can assign itself the token and begin replanning with the new constraints. When a bid message is received, it is added to the list of bids to check after the next plan update. In addition, the assumptions on the network allow all agents to receive all messages sent, maintaining data consistency across the team.

\section{Results and Future Work}
For the DMA-RRT algorithm, i.e. using merit-based token passing. Each agent computes its path cost, and thus its bid, based on the path’s length in seconds (i.e. travel time). Adding the intelligent coordination policy yields significantly better results. For a 3-agent system the planning was observed just as described in section 5 above. Only one agent plans in one iteration, this strategy makes computation easier and the constraints are not that complex because every agent plans according to current plans, so if all the constrains are satisfied in current time step, then they will satisfy in future time steps. Therefore, these algorithms are capable of combining the CL-RRT path planner with a new coordination strategy, merit-based token passing, to improve team performance while ensuring all constraints are satisfied.

I want to extend this algorithm for dynamic obstacles in the environment. Also for larger teams I want to implement this algorithm using better cost functions and more than one agent planning in a time step.



\begin{thebibliography}{9}
\bibitem{c1} V. Desaraju and J. How, "Decentralized path planning for multi-agent teams in complex environments using rapidly-exploring random trees," in Proc. of the IEEE Int. Conf. on Robotics and Autom. (ICRA), 2011.
\bibitem{c2} M. Bennewitz, W. Burgard, G. Cielniak, and S. Thrun. Learning motion patterns of people for compliant robot motion. Int. J. of Robotics Res., 24:31–48, 2005.
\bibitem{c3} M. Flint, M. Polycarpou, and E. Fernandez-Gaucherand, “Cooperative pathplanning
for autonomous vehicles using dynamic programming,” in Proceedings of the IFAC World Congress, (Barcelona, Spain), 2002.
\bibitem{c4} P. E. Hart, N. J. Nilsson, and B. Raphael, “A formal basis for the heuristic determination of minimum cost paths,” IEEE Transactions of Systems Science and Cybernetics, vol. 4, pp. 100–106, July 1968.
\bibitem{c5} M. Likhachev and A. Stentz, “R* search,” in Proceedings of the AAAI Conference
on Artificial Intelligence, pp. 344–350, 2008.
\bibitem{c6} S. Koenig and M. Likhachev, “D* lite,” in Proceedings AAAI National Conference
on Artificial Intelligence, pp. 476–483, 2002.
\bibitem{c7} M. Likhachev, Search-based Planning for Large Dynamic Environments. PhD
thesis, Carnegie Mellon University, School of Computer Science, Pittsburgh, PA,
September 2005.

\bibitem{c8} D. Bertsekas, Dynamic Programming and Optimal Control. Athena Scientific, 2005.
\bibitem{c9} T. Schouwenaars, B. de Moor, E. Feron, and J. P. How, “Mixed integer programming
for multi-vehicle path planning,” in Proceedings of the European Control Conference, (Porto, Portugal), pp. 2603–2608, European Union Control Association, September 2001.

\bibitem{c10} W. B. Dunbar and R. M. Murray, “Model predictive control of coordinated multi-vehicle formations,” in Proceedings of the IEEE Conference on Decision and Control, (Las Vegas, NV), pp. 4631–4636, December 2002 2002.
\bibitem{c11} Y. Kuwata, S. Karaman, J. Teo, E. Frazzoli, J. P. How, and G. Fiore, “Realtime
motion planning with applications to autonomous urban driving,” IEEE Transactions on Control Systems Technology, vol. 17, pp. 1105 – 1118, Sept. 2009.
\bibitem{c12} S. M. LaValle, Planning Algorithms. Cambridge, U.K.: Cambridge University
Press, 2006.

\end{thebibliography}
\end{document}
